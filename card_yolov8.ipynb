{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "323a4995-14a3-4666-a69e-b8446472f606",
   "metadata": {},
   "source": [
    "## Downloading dataset from Roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b1d57e38-353e-4536-a428-540415327991",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n",
      "Dependency ultralytics==8.0.196 is required but found version=8.1.11, to fix: `pip install ultralytics==8.0.196`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Playing-Cards-4 to yolov8:: 100%|██████████| 2119661/2119661 [00:41<00:00, 50662.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Playing-Cards-4 in yolov8:: 100%|██████████| 48478/48478 [00:09<00:00, 5162.82it/s] \n"
     ]
    }
   ],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"GXx1mInWvTfDrKB73mbr\")\n",
    "project = rf.workspace(\"augmented-startups\").project(\"playing-cards-ow27d\")\n",
    "dataset = project.version(4).download(\"yolov8\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "99d33908-383b-4dfe-a3ab-bf33cabb07da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83331c64-d4ab-4a8d-bf53-53af0fe2a0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-summary\n",
      "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: torch-summary\n",
      "Successfully installed torch-summary-1.4.5\n"
     ]
    }
   ],
   "source": [
    "# !pip install torch-summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6c1e26cb-10b5-4145-a240-9e559702af09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv8n summary: 225 layers, 3157200 parameters, 0 gradients, 8.9 GFLOPs\n",
      "(225, 3157200, 0, 8.8575488)\n"
     ]
    }
   ],
   "source": [
    "# We will use yolov8n because it is smallest model size, and should be sufficient for non-complex task such as this.\n",
    "from ultralytics import YOLO\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "print(model.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef3158a7-45cf-4bc2-92a1-77ddeb83c141",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.model.0.conv.weight\n",
      "model.model.0.bn.weight\n",
      "model.model.0.bn.bias\n",
      "model.model.1.conv.weight\n",
      "model.model.1.bn.weight\n",
      "model.model.1.bn.bias\n",
      "model.model.2.cv1.conv.weight\n",
      "model.model.2.cv1.bn.weight\n",
      "model.model.2.cv1.bn.bias\n",
      "model.model.2.cv2.conv.weight\n",
      "model.model.2.cv2.bn.weight\n",
      "model.model.2.cv2.bn.bias\n",
      "model.model.2.m.0.cv1.conv.weight\n",
      "model.model.2.m.0.cv1.bn.weight\n",
      "model.model.2.m.0.cv1.bn.bias\n",
      "model.model.2.m.0.cv2.conv.weight\n",
      "model.model.2.m.0.cv2.bn.weight\n",
      "model.model.2.m.0.cv2.bn.bias\n",
      "model.model.3.conv.weight\n",
      "model.model.3.bn.weight\n",
      "model.model.3.bn.bias\n",
      "model.model.4.cv1.conv.weight\n",
      "model.model.4.cv1.bn.weight\n",
      "model.model.4.cv1.bn.bias\n",
      "model.model.4.cv2.conv.weight\n",
      "model.model.4.cv2.bn.weight\n",
      "model.model.4.cv2.bn.bias\n",
      "model.model.4.m.0.cv1.conv.weight\n",
      "model.model.4.m.0.cv1.bn.weight\n",
      "model.model.4.m.0.cv1.bn.bias\n",
      "model.model.4.m.0.cv2.conv.weight\n",
      "model.model.4.m.0.cv2.bn.weight\n",
      "model.model.4.m.0.cv2.bn.bias\n",
      "model.model.4.m.1.cv1.conv.weight\n",
      "model.model.4.m.1.cv1.bn.weight\n",
      "model.model.4.m.1.cv1.bn.bias\n",
      "model.model.4.m.1.cv2.conv.weight\n",
      "model.model.4.m.1.cv2.bn.weight\n",
      "model.model.4.m.1.cv2.bn.bias\n",
      "model.model.5.conv.weight\n",
      "model.model.5.bn.weight\n",
      "model.model.5.bn.bias\n",
      "model.model.6.cv1.conv.weight\n",
      "model.model.6.cv1.bn.weight\n",
      "model.model.6.cv1.bn.bias\n",
      "model.model.6.cv2.conv.weight\n",
      "model.model.6.cv2.bn.weight\n",
      "model.model.6.cv2.bn.bias\n",
      "model.model.6.m.0.cv1.conv.weight\n",
      "model.model.6.m.0.cv1.bn.weight\n",
      "model.model.6.m.0.cv1.bn.bias\n",
      "model.model.6.m.0.cv2.conv.weight\n",
      "model.model.6.m.0.cv2.bn.weight\n",
      "model.model.6.m.0.cv2.bn.bias\n",
      "model.model.6.m.1.cv1.conv.weight\n",
      "model.model.6.m.1.cv1.bn.weight\n",
      "model.model.6.m.1.cv1.bn.bias\n",
      "model.model.6.m.1.cv2.conv.weight\n",
      "model.model.6.m.1.cv2.bn.weight\n",
      "model.model.6.m.1.cv2.bn.bias\n",
      "model.model.7.conv.weight\n",
      "model.model.7.bn.weight\n",
      "model.model.7.bn.bias\n",
      "model.model.8.cv1.conv.weight\n",
      "model.model.8.cv1.bn.weight\n",
      "model.model.8.cv1.bn.bias\n",
      "model.model.8.cv2.conv.weight\n",
      "model.model.8.cv2.bn.weight\n",
      "model.model.8.cv2.bn.bias\n",
      "model.model.8.m.0.cv1.conv.weight\n",
      "model.model.8.m.0.cv1.bn.weight\n",
      "model.model.8.m.0.cv1.bn.bias\n",
      "model.model.8.m.0.cv2.conv.weight\n",
      "model.model.8.m.0.cv2.bn.weight\n",
      "model.model.8.m.0.cv2.bn.bias\n",
      "model.model.9.cv1.conv.weight\n",
      "model.model.9.cv1.bn.weight\n",
      "model.model.9.cv1.bn.bias\n",
      "model.model.9.cv2.conv.weight\n",
      "model.model.9.cv2.bn.weight\n",
      "model.model.9.cv2.bn.bias\n",
      "model.model.12.cv1.conv.weight\n",
      "model.model.12.cv1.bn.weight\n",
      "model.model.12.cv1.bn.bias\n",
      "model.model.12.cv2.conv.weight\n",
      "model.model.12.cv2.bn.weight\n",
      "model.model.12.cv2.bn.bias\n",
      "model.model.12.m.0.cv1.conv.weight\n",
      "model.model.12.m.0.cv1.bn.weight\n",
      "model.model.12.m.0.cv1.bn.bias\n",
      "model.model.12.m.0.cv2.conv.weight\n",
      "model.model.12.m.0.cv2.bn.weight\n",
      "model.model.12.m.0.cv2.bn.bias\n",
      "model.model.15.cv1.conv.weight\n",
      "model.model.15.cv1.bn.weight\n",
      "model.model.15.cv1.bn.bias\n",
      "model.model.15.cv2.conv.weight\n",
      "model.model.15.cv2.bn.weight\n",
      "model.model.15.cv2.bn.bias\n",
      "model.model.15.m.0.cv1.conv.weight\n",
      "model.model.15.m.0.cv1.bn.weight\n",
      "model.model.15.m.0.cv1.bn.bias\n",
      "model.model.15.m.0.cv2.conv.weight\n",
      "model.model.15.m.0.cv2.bn.weight\n",
      "model.model.15.m.0.cv2.bn.bias\n",
      "model.model.16.conv.weight\n",
      "model.model.16.bn.weight\n",
      "model.model.16.bn.bias\n",
      "model.model.18.cv1.conv.weight\n",
      "model.model.18.cv1.bn.weight\n",
      "model.model.18.cv1.bn.bias\n",
      "model.model.18.cv2.conv.weight\n",
      "model.model.18.cv2.bn.weight\n",
      "model.model.18.cv2.bn.bias\n",
      "model.model.18.m.0.cv1.conv.weight\n",
      "model.model.18.m.0.cv1.bn.weight\n",
      "model.model.18.m.0.cv1.bn.bias\n",
      "model.model.18.m.0.cv2.conv.weight\n",
      "model.model.18.m.0.cv2.bn.weight\n",
      "model.model.18.m.0.cv2.bn.bias\n",
      "model.model.19.conv.weight\n",
      "model.model.19.bn.weight\n",
      "model.model.19.bn.bias\n",
      "model.model.21.cv1.conv.weight\n",
      "model.model.21.cv1.bn.weight\n",
      "model.model.21.cv1.bn.bias\n",
      "model.model.21.cv2.conv.weight\n",
      "model.model.21.cv2.bn.weight\n",
      "model.model.21.cv2.bn.bias\n",
      "model.model.21.m.0.cv1.conv.weight\n",
      "model.model.21.m.0.cv1.bn.weight\n",
      "model.model.21.m.0.cv1.bn.bias\n",
      "model.model.21.m.0.cv2.conv.weight\n",
      "model.model.21.m.0.cv2.bn.weight\n",
      "model.model.21.m.0.cv2.bn.bias\n",
      "model.model.22.cv2.0.0.conv.weight\n",
      "model.model.22.cv2.0.0.bn.weight\n",
      "model.model.22.cv2.0.0.bn.bias\n",
      "model.model.22.cv2.0.1.conv.weight\n",
      "model.model.22.cv2.0.1.bn.weight\n",
      "model.model.22.cv2.0.1.bn.bias\n",
      "model.model.22.cv2.0.2.weight\n",
      "model.model.22.cv2.0.2.bias\n",
      "model.model.22.cv2.1.0.conv.weight\n",
      "model.model.22.cv2.1.0.bn.weight\n",
      "model.model.22.cv2.1.0.bn.bias\n",
      "model.model.22.cv2.1.1.conv.weight\n",
      "model.model.22.cv2.1.1.bn.weight\n",
      "model.model.22.cv2.1.1.bn.bias\n",
      "model.model.22.cv2.1.2.weight\n",
      "model.model.22.cv2.1.2.bias\n",
      "model.model.22.cv2.2.0.conv.weight\n",
      "model.model.22.cv2.2.0.bn.weight\n",
      "model.model.22.cv2.2.0.bn.bias\n",
      "model.model.22.cv2.2.1.conv.weight\n",
      "model.model.22.cv2.2.1.bn.weight\n",
      "model.model.22.cv2.2.1.bn.bias\n",
      "model.model.22.cv2.2.2.weight\n",
      "model.model.22.cv2.2.2.bias\n",
      "model.model.22.cv3.0.0.conv.weight\n",
      "model.model.22.cv3.0.0.bn.weight\n",
      "model.model.22.cv3.0.0.bn.bias\n",
      "model.model.22.cv3.0.1.conv.weight\n",
      "model.model.22.cv3.0.1.bn.weight\n",
      "model.model.22.cv3.0.1.bn.bias\n",
      "model.model.22.cv3.0.2.weight\n",
      "model.model.22.cv3.0.2.bias\n",
      "model.model.22.cv3.1.0.conv.weight\n",
      "model.model.22.cv3.1.0.bn.weight\n",
      "model.model.22.cv3.1.0.bn.bias\n",
      "model.model.22.cv3.1.1.conv.weight\n",
      "model.model.22.cv3.1.1.bn.weight\n",
      "model.model.22.cv3.1.1.bn.bias\n",
      "model.model.22.cv3.1.2.weight\n",
      "model.model.22.cv3.1.2.bias\n",
      "model.model.22.cv3.2.0.conv.weight\n",
      "model.model.22.cv3.2.0.bn.weight\n",
      "model.model.22.cv3.2.0.bn.bias\n",
      "model.model.22.cv3.2.1.conv.weight\n",
      "model.model.22.cv3.2.1.bn.weight\n",
      "model.model.22.cv3.2.1.bn.bias\n",
      "model.model.22.cv3.2.2.weight\n",
      "model.model.22.cv3.2.2.bias\n",
      "model.model.22.dfl.conv.weight\n"
     ]
    }
   ],
   "source": [
    "# Check all the layers to identify the layers to freeze during transfer learning\n",
    "for k, v in model.named_parameters():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "538f6eb8-c819-4fcd-b994-fd21683d919a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !yolo train model=yolov8n.pt data='data.yaml' epochs=3 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118327ab-a796-4861-926f-80dadd7920ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=data2.yaml, epochs=50, time=None, patience=50, batch=64, imgsz=640, save=True, save_period=5, cache=False, device=None, workers=8, project=None, name=train_card_v3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=18, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train_card_v3\n",
      "2024-02-12 08:26:45.411518: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-12 08:26:45.411567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-12 08:26:45.413182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Overriding model.yaml nc=80 with nc=52\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    761452  ultralytics.nn.modules.head.Detect           [52, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3020988 parameters, 3020972 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "WARNING ⚠️ Comet installed but not initialized correctly, not logging this run. Comet.ml requires an API key. Please provide as the first argument to Experiment(api_key) or as an environment variable named COMET_API_KEY \n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train_card_v3', view at http://localhost:6006/\n",
      "Freezing layer 'model.0.conv.weight'\n",
      "Freezing layer 'model.0.bn.weight'\n",
      "Freezing layer 'model.0.bn.bias'\n",
      "Freezing layer 'model.1.conv.weight'\n",
      "Freezing layer 'model.1.bn.weight'\n",
      "Freezing layer 'model.1.bn.bias'\n",
      "Freezing layer 'model.2.cv1.conv.weight'\n",
      "Freezing layer 'model.2.cv1.bn.weight'\n",
      "Freezing layer 'model.2.cv1.bn.bias'\n",
      "Freezing layer 'model.2.cv2.conv.weight'\n",
      "Freezing layer 'model.2.cv2.bn.weight'\n",
      "Freezing layer 'model.2.cv2.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.2.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.2.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.3.conv.weight'\n",
      "Freezing layer 'model.3.bn.weight'\n",
      "Freezing layer 'model.3.bn.bias'\n",
      "Freezing layer 'model.4.cv1.conv.weight'\n",
      "Freezing layer 'model.4.cv1.bn.weight'\n",
      "Freezing layer 'model.4.cv1.bn.bias'\n",
      "Freezing layer 'model.4.cv2.conv.weight'\n",
      "Freezing layer 'model.4.cv2.bn.weight'\n",
      "Freezing layer 'model.4.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.4.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.4.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.5.conv.weight'\n",
      "Freezing layer 'model.5.bn.weight'\n",
      "Freezing layer 'model.5.bn.bias'\n",
      "Freezing layer 'model.6.cv1.conv.weight'\n",
      "Freezing layer 'model.6.cv1.bn.weight'\n",
      "Freezing layer 'model.6.cv1.bn.bias'\n",
      "Freezing layer 'model.6.cv2.conv.weight'\n",
      "Freezing layer 'model.6.cv2.bn.weight'\n",
      "Freezing layer 'model.6.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv1.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv1.bn.bias'\n",
      "Freezing layer 'model.6.m.1.cv2.conv.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.weight'\n",
      "Freezing layer 'model.6.m.1.cv2.bn.bias'\n",
      "Freezing layer 'model.7.conv.weight'\n",
      "Freezing layer 'model.7.bn.weight'\n",
      "Freezing layer 'model.7.bn.bias'\n",
      "Freezing layer 'model.8.cv1.conv.weight'\n",
      "Freezing layer 'model.8.cv1.bn.weight'\n",
      "Freezing layer 'model.8.cv1.bn.bias'\n",
      "Freezing layer 'model.8.cv2.conv.weight'\n",
      "Freezing layer 'model.8.cv2.bn.weight'\n",
      "Freezing layer 'model.8.cv2.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.8.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.8.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.9.cv1.conv.weight'\n",
      "Freezing layer 'model.9.cv1.bn.weight'\n",
      "Freezing layer 'model.9.cv1.bn.bias'\n",
      "Freezing layer 'model.9.cv2.conv.weight'\n",
      "Freezing layer 'model.9.cv2.bn.weight'\n",
      "Freezing layer 'model.9.cv2.bn.bias'\n",
      "Freezing layer 'model.12.cv1.conv.weight'\n",
      "Freezing layer 'model.12.cv1.bn.weight'\n",
      "Freezing layer 'model.12.cv1.bn.bias'\n",
      "Freezing layer 'model.12.cv2.conv.weight'\n",
      "Freezing layer 'model.12.cv2.bn.weight'\n",
      "Freezing layer 'model.12.cv2.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.12.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.12.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.15.cv1.conv.weight'\n",
      "Freezing layer 'model.15.cv1.bn.weight'\n",
      "Freezing layer 'model.15.cv1.bn.bias'\n",
      "Freezing layer 'model.15.cv2.conv.weight'\n",
      "Freezing layer 'model.15.cv2.bn.weight'\n",
      "Freezing layer 'model.15.cv2.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv1.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv1.bn.bias'\n",
      "Freezing layer 'model.15.m.0.cv2.conv.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.weight'\n",
      "Freezing layer 'model.15.m.0.cv2.bn.bias'\n",
      "Freezing layer 'model.16.conv.weight'\n",
      "Freezing layer 'model.16.bn.weight'\n",
      "Freezing layer 'model.16.bn.bias'\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/jupyter/datasets/Playing-Cards-4/train/labels... 21203 ima\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/jupyter/datasets/Playing-Cards-4/train/labels.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/datasets/Playing-Cards-4/valid/labels... 2020 images\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jupyter/datasets/Playing-Cards-4/valid/labels.cache\n",
      "Plotting labels to runs/detect/train_card_v3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train_card_v3\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/50      4.48G      1.796      4.902      1.262        128        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       2020       8080      0.057      0.141     0.0432     0.0241\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/50      4.03G      1.581      3.399      1.103        117        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       2020       8080      0.214      0.411       0.22      0.125\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/50      3.92G       1.55      2.742      1.071        128        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       2020       8080      0.381      0.519      0.434      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/50      3.98G       1.52      2.194      1.057         87        640: 1\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all       2020       8080      0.559      0.646      0.656       0.39\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/50      3.95G      1.487      1.939      1.041        374        640:  "
     ]
    }
   ],
   "source": [
    "!yolo train model=yolov8n.pt data='data2.yaml' epochs=50 imgsz=640 batch=64 name=train_card_v3 verbose=True freeze=18 plots=True save_period=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c6a382-4e28-4353-9170-66960e742bbc",
   "metadata": {},
   "source": [
    "## Test Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84f0c789-3872-48d2-8761-f7faa43324bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3015788 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/jupyter/datasets/Cards-and-such-1/test/labels... 990 images,\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/jupyter/datasets/Cards-and-such-1/test/labels.cache\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  m\n",
      "                   all        990       3960      0.878      0.874      0.944       0.79\n",
      "                   10C        990         76      0.976      0.987      0.994      0.842\n",
      "                   10D        990         71      0.971      0.972      0.987      0.833\n",
      "                   10H        990         53      0.898      0.997      0.989      0.835\n",
      "                   10S        990         83      0.954      0.988      0.991      0.834\n",
      "                    2C        990         82      0.798      0.869       0.92      0.773\n",
      "                    2D        990         68      0.667      0.882      0.872      0.724\n",
      "                    2H        990         83      0.938      0.909      0.977      0.823\n",
      "                    2S        990         82      0.739      0.831      0.906      0.776\n",
      "                    3C        990         73      0.892      0.877      0.955      0.826\n",
      "                    3D        990         72      0.792      0.903      0.925      0.783\n",
      "                    3H        990         88      0.986      0.776      0.959      0.782\n",
      "                    3S        990         92      0.874      0.826      0.918      0.768\n",
      "                    4C        990         67          1      0.967      0.988      0.815\n",
      "                    4D        990        100      0.845       0.94      0.947      0.769\n",
      "                    4H        990         84          1      0.797      0.985      0.823\n",
      "                    4S        990         89      0.988      0.889      0.991      0.828\n",
      "                    5C        990         94          1      0.775      0.984      0.824\n",
      "                    5D        990         68      0.775      0.882      0.912      0.774\n",
      "                    5H        990         62       0.96      0.772       0.95      0.811\n",
      "                    5S        990         76      0.981      0.691      0.931      0.757\n",
      "                    6C        990         67      0.618      0.869      0.801      0.691\n",
      "                    6D        990         68      0.708      0.838      0.856      0.737\n",
      "                    6H        990         81      0.848      0.864      0.927      0.803\n",
      "                    6S        990         69      0.577      0.797      0.802      0.678\n",
      "                    7C        990         47          1      0.969      0.993      0.861\n",
      "                    7D        990         84      0.882          1      0.984       0.81\n",
      "                    7H        990         75      0.985      0.899      0.987       0.85\n",
      "                    7S        990         87      0.955      0.965      0.989      0.857\n",
      "                    8C        990         63      0.846       0.81      0.892      0.757\n",
      "                    8D        990         98      0.897      0.867      0.938      0.771\n",
      "                    8H        990         85      0.859      0.718      0.929      0.783\n",
      "                    8S        990         81      0.841      0.781      0.907      0.775\n",
      "                    9C        990         61      0.815      0.951      0.948      0.805\n",
      "                    9D        990         63      0.817      0.921      0.929      0.788\n",
      "                    9H        990         75      0.945      0.893      0.976      0.843\n",
      "                    9S        990         82       0.93       0.97      0.985      0.859\n",
      "                    AC        990         66      0.908      0.894      0.957      0.777\n",
      "                    AD        990         85      0.907      0.953       0.97      0.819\n",
      "                    AH        990         67      0.955          1      0.994      0.826\n",
      "                    AS        990         88      0.765      0.963        0.9      0.746\n",
      "                    JC        990         65      0.945      0.892      0.978       0.77\n",
      "                    JD        990         79      0.828      0.937      0.949      0.819\n",
      "                    JH        990         61      0.771      0.773      0.896      0.743\n",
      "                    JS        990         90      0.874      0.922      0.972      0.811\n",
      "                    KC        990         68      0.979      0.684       0.97      0.646\n",
      "                    KD        990         67      0.745      0.821       0.88      0.767\n",
      "                    KH        990         81      0.957      0.549      0.893      0.769\n",
      "                    KS        990         65      0.914      0.846      0.942      0.716\n",
      "                    QC        990         72      0.956      0.986      0.987      0.864\n",
      "                    QD        990         99      0.775      0.848      0.919      0.828\n",
      "                    QH        990         81      0.913       0.79       0.95      0.677\n",
      "                    QS        990         77      0.896      0.948      0.985      0.855\n",
      "Speed: 0.4ms preprocess, 4.2ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/val4\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/val\n"
     ]
    }
   ],
   "source": [
    "!yolo val model=\"runs/detect/train_card_v1/weights/best.pt\" data=data-test2.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a66a5db7-2646-4828-b0f7-8a63b617ed24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3015788 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /home/jupyter/datasets/Cards-and-such-1/test/images/001771721_jpg.rf.8c07774fbd271d18594535b6daf15688.jpg: 640x640 2 7Ss, 1 8S, 1 AH, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 507.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict3\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Running on a sample image\n",
    "!yolo predict model=\"runs/detect/train_card_v3/weights/best.pt\" source=\"datasets/Cards-and-such-1/test/images/001771721_jpg.rf.8c07774fbd271d18594535b6daf15688.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88afcd-e734-4d8d-a780-cfd2699c33d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running Prediction on Video Footage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3518073f-2d8f-4ba3-82ec-7d36f301394e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://www.youtube.com/watch?v=5F6j4e1C4Zk\n",
      "[youtube] 5F6j4e1C4Zk: Downloading webpage\n",
      "[youtube] 5F6j4e1C4Zk: Downloading ios player API JSON\n",
      "[youtube] 5F6j4e1C4Zk: Downloading android player API JSON\n",
      "[youtube] 5F6j4e1C4Zk: Downloading m3u8 information\n",
      "[info] 5F6j4e1C4Zk: Downloading 1 format(s): 22\n",
      "[download] Destination: datasets/youtube_vid/How to Stack Playing Cards ｜ WIRED.mp4\n",
      "[download] 100% of   18.02MiB in 00:00:01 at 13.73MiB/s  \n"
     ]
    }
   ],
   "source": [
    "from yt_dlp import YoutubeDL\n",
    "\n",
    "# URLS = [\"https://www.youtube.com/watch?v=5F6j4e1C4Zk\"]\n",
    "URLS = [\"https://www.youtube.com/watch?v=G_So72lFNIU\"]\n",
    "\n",
    "dl_ops = {\n",
    "  'outtmpl': 'datasets/youtube_vid/%(title)s.%(ext)s'\n",
    "}\n",
    "\n",
    "with YoutubeDL(dl_ops) as ydl:\n",
    "    ydl.download(URLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8739a2e1-2743-491b-a4da-fa064375007c",
   "metadata": {},
   "source": [
    "### Frame Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8d7bfc84-828b-4462-b74e-adc535210bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "\n",
    "def video_to_frames(input_loc, output_loc, num_frames=10):\n",
    "    \"\"\"Function to extract frames from input video file\n",
    "    and save them as separate frames in an output directory.\n",
    "    Args:\n",
    "        input_loc: Input video file.\n",
    "        output_loc: Output directory to save the frames.\n",
    "        num_frames: Desired number of equally spaced frames.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(output_loc)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Log the time\n",
    "    time_start = time.time()\n",
    "\n",
    "    # Start capturing the feed\n",
    "    cap = cv2.VideoCapture(input_loc)\n",
    "\n",
    "    # Find the number of frames\n",
    "    video_length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) - 1\n",
    "    print(\"Number of available frames: \", video_length)\n",
    "    print(\"Number of printing frames: \", min(num_frames, video_length))\n",
    "\n",
    "    # Calculate the step size to get equally spaced frames\n",
    "    step_size = max(1, video_length // num_frames)\n",
    "\n",
    "    count = 0\n",
    "    print(\"Converting video..\\n\")\n",
    "\n",
    "    # Start converting the video\n",
    "    while cap.isOpened():\n",
    "        # Extract the frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "\n",
    "        # Check if the current frame should be captured\n",
    "        if count % step_size == 0:\n",
    "            # Write the results back to the output location.\n",
    "            cv2.imwrite(output_loc + \"/%#05d.jpg\" % (count//step_size + 1), frame)\n",
    "\n",
    "        count = count + 1\n",
    "\n",
    "        # If there are no more frames left\n",
    "        if count > video_length - 1:\n",
    "            # Log the time again\n",
    "            time_end = time.time()\n",
    "            # Release the feed\n",
    "            cap.release()\n",
    "            # Print stats\n",
    "            print(\"Done extracting frames.\\n%d frames extracted\" % (count//step_size))\n",
    "            print(\"It took %d seconds for conversion.\" % (time_end - time_start))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d30c5ec3-def7-4ae1-8339-3202d8e0fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How to Stack Playing Cards ｜ WIRED.mp4']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_loc = \"datasets/youtube_vid/\"\n",
    "video = os.listdir(video_loc)\n",
    "video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "937f4ce9-fde2-4fe3-8a3f-f4feb679e27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available frames:  6612\n",
      "Number of printing frames:  50\n",
      "Converting video..\n",
      "\n",
      "Done extracting frames.\n",
      "50 frames extracted\n",
      "It took 16 seconds for conversion.\n"
     ]
    }
   ],
   "source": [
    "input_loc = video_loc + video[0]\n",
    "output_loc = \"datasets/youtube_frames\"\n",
    "\n",
    "video_to_frames(input_loc, output_loc,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b29e6ee-174a-44b1-a875-e9fb4f02fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.2.0+cu121 CUDA:0 (Tesla T4, 14931MiB)\n",
      "Model summary (fused): 168 layers, 3015788 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/51 /home/jupyter/datasets/youtube_frames/00001.jpg: 384x640 (no detections), 287.5ms\n",
      "image 2/51 /home/jupyter/datasets/youtube_frames/00002.jpg: 384x640 1 AS, 12.3ms\n",
      "image 3/51 /home/jupyter/datasets/youtube_frames/00003.jpg: 384x640 (no detections), 7.0ms\n",
      "image 4/51 /home/jupyter/datasets/youtube_frames/00004.jpg: 384x640 (no detections), 7.2ms\n",
      "image 5/51 /home/jupyter/datasets/youtube_frames/00005.jpg: 384x640 1 7D, 7.6ms\n",
      "image 6/51 /home/jupyter/datasets/youtube_frames/00006.jpg: 384x640 (no detections), 9.3ms\n",
      "image 7/51 /home/jupyter/datasets/youtube_frames/00007.jpg: 384x640 (no detections), 11.5ms\n",
      "image 8/51 /home/jupyter/datasets/youtube_frames/00008.jpg: 384x640 (no detections), 10.1ms\n",
      "image 9/51 /home/jupyter/datasets/youtube_frames/00009.jpg: 384x640 (no detections), 6.5ms\n",
      "image 10/51 /home/jupyter/datasets/youtube_frames/00010.jpg: 384x640 (no detections), 6.8ms\n",
      "image 11/51 /home/jupyter/datasets/youtube_frames/00011.jpg: 384x640 1 7S, 6.9ms\n",
      "image 12/51 /home/jupyter/datasets/youtube_frames/00012.jpg: 384x640 (no detections), 7.0ms\n",
      "image 13/51 /home/jupyter/datasets/youtube_frames/00013.jpg: 384x640 (no detections), 7.9ms\n",
      "image 14/51 /home/jupyter/datasets/youtube_frames/00014.jpg: 384x640 1 6H, 1 8H, 1 9H, 7.7ms\n",
      "image 15/51 /home/jupyter/datasets/youtube_frames/00015.jpg: 384x640 (no detections), 7.5ms\n",
      "image 16/51 /home/jupyter/datasets/youtube_frames/00016.jpg: 384x640 (no detections), 7.4ms\n",
      "image 17/51 /home/jupyter/datasets/youtube_frames/00017.jpg: 384x640 (no detections), 7.3ms\n",
      "image 18/51 /home/jupyter/datasets/youtube_frames/00018.jpg: 384x640 (no detections), 9.7ms\n",
      "image 19/51 /home/jupyter/datasets/youtube_frames/00019.jpg: 384x640 (no detections), 11.6ms\n",
      "image 20/51 /home/jupyter/datasets/youtube_frames/00020.jpg: 384x640 (no detections), 12.1ms\n",
      "image 21/51 /home/jupyter/datasets/youtube_frames/00021.jpg: 384x640 (no detections), 6.8ms\n",
      "image 22/51 /home/jupyter/datasets/youtube_frames/00022.jpg: 384x640 (no detections), 6.8ms\n",
      "image 23/51 /home/jupyter/datasets/youtube_frames/00023.jpg: 384x640 (no detections), 6.8ms\n",
      "image 24/51 /home/jupyter/datasets/youtube_frames/00024.jpg: 384x640 (no detections), 6.8ms\n",
      "image 25/51 /home/jupyter/datasets/youtube_frames/00025.jpg: 384x640 (no detections), 7.3ms\n",
      "image 26/51 /home/jupyter/datasets/youtube_frames/00026.jpg: 384x640 1 3H, 7.3ms\n",
      "image 27/51 /home/jupyter/datasets/youtube_frames/00027.jpg: 384x640 (no detections), 7.4ms\n",
      "image 28/51 /home/jupyter/datasets/youtube_frames/00028.jpg: 384x640 (no detections), 7.3ms\n",
      "image 29/51 /home/jupyter/datasets/youtube_frames/00029.jpg: 384x640 (no detections), 7.1ms\n",
      "image 30/51 /home/jupyter/datasets/youtube_frames/00030.jpg: 384x640 1 3H, 1 6S, 7.4ms\n",
      "image 31/51 /home/jupyter/datasets/youtube_frames/00031.jpg: 384x640 (no detections), 7.2ms\n",
      "image 32/51 /home/jupyter/datasets/youtube_frames/00032.jpg: 384x640 (no detections), 11.4ms\n",
      "image 33/51 /home/jupyter/datasets/youtube_frames/00033.jpg: 384x640 (no detections), 7.8ms\n",
      "image 34/51 /home/jupyter/datasets/youtube_frames/00034.jpg: 384x640 1 7D, 7.3ms\n",
      "image 35/51 /home/jupyter/datasets/youtube_frames/00035.jpg: 384x640 (no detections), 7.1ms\n",
      "image 36/51 /home/jupyter/datasets/youtube_frames/00036.jpg: 384x640 (no detections), 6.4ms\n",
      "image 37/51 /home/jupyter/datasets/youtube_frames/00037.jpg: 384x640 (no detections), 6.8ms\n",
      "image 38/51 /home/jupyter/datasets/youtube_frames/00038.jpg: 384x640 (no detections), 7.0ms\n",
      "image 39/51 /home/jupyter/datasets/youtube_frames/00039.jpg: 384x640 (no detections), 7.5ms\n",
      "image 40/51 /home/jupyter/datasets/youtube_frames/00040.jpg: 384x640 (no detections), 7.2ms\n",
      "image 41/51 /home/jupyter/datasets/youtube_frames/00041.jpg: 384x640 (no detections), 7.3ms\n",
      "image 42/51 /home/jupyter/datasets/youtube_frames/00042.jpg: 384x640 (no detections), 10.9ms\n",
      "image 43/51 /home/jupyter/datasets/youtube_frames/00043.jpg: 384x640 (no detections), 11.6ms\n",
      "image 44/51 /home/jupyter/datasets/youtube_frames/00044.jpg: 384x640 (no detections), 8.1ms\n",
      "image 45/51 /home/jupyter/datasets/youtube_frames/00045.jpg: 384x640 (no detections), 6.9ms\n",
      "image 46/51 /home/jupyter/datasets/youtube_frames/00046.jpg: 384x640 (no detections), 6.7ms\n",
      "image 47/51 /home/jupyter/datasets/youtube_frames/00047.jpg: 384x640 (no detections), 6.7ms\n",
      "image 48/51 /home/jupyter/datasets/youtube_frames/00048.jpg: 384x640 (no detections), 7.8ms\n",
      "image 49/51 /home/jupyter/datasets/youtube_frames/00049.jpg: 384x640 (no detections), 7.6ms\n",
      "image 50/51 /home/jupyter/datasets/youtube_frames/00050.jpg: 384x640 (no detections), 7.3ms\n",
      "image 51/51 /home/jupyter/datasets/youtube_frames/00051.jpg: 384x640 (no detections), 7.2ms\n",
      "Speed: 2.0ms preprocess, 13.4ms inference, 40.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict2\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=\"runs/detect/train_card_v1/weights/best.pt\" source='datasets/youtube_frames'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab06d3c-29fe-4c4a-91c2-cf176642bdec",
   "metadata": {},
   "source": [
    "## Using Video Footage to Self-Generate More Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f28842-1827-459f-9a2f-c52747916af6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m116",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-15:m116"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
